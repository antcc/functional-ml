\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=magenta}
\setlength{\parindent}{0in}
\usepackage[margin=0.8in]{geometry}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{engord}
\usepackage{parskip}
% \usepackage{minted}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[compact]{titlesec}
\usepackage[center]{caption}
\usepackage{placeins}
\usepackage{color}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{pdfpages}
% \titlespacing*{\subsection}{0pt}{5.5ex}{3.3ex}
% \titlespacing*{\section}{0pt}{5.5ex}{1ex}
\author{Luis Antonio Ortega Andrés\\Antonio Coín Castro}
\date{\today}
\title{Kernel Methods\\\medskip
\large Homework 2}
\hypersetup{
 pdfauthor={Luis Antonio Ortega Andrés, Antonio Coín Castro},
 pdftitle={HW02},
 pdfkeywords={},
 pdfsubject={},
 pdflang={English}}

% \usemintedstyle{bw}

\begin{document}

\maketitle

\section{TODO}

Revisar cuaderno.

Aclarar que eliminamos las componentes no nulas.

\section*{Kernel PCA}

\emph{Why do the projections onto the first two KPCA principal components look different for the Sklearn and our implementation? Is any of the two incorrect?}

Both representations are correct and valid, what makes them different is the decisions made at normalizing the eigenvectors. 

These vectors are returned with \(\|\cdot \|_{\mathcal{L}_2} = 1\) in \( \mathbb{R}^N \)  by Numpy and Scipy via specialized functions such as \texttt{eig} and \texttt{eigh} (version for symmetric cases). However, the kernel method needs these vectors to be unitary in the subjacent Hilbert space, not \( \mathbb{R}^N \). This can be achieved by ensuring that their \( \mathcal{L}_2 \)-norm in \( \mathbb{R}^N \)  equals the inverse of the square root of their corresponding eigenvalues \( 1/\sqrt{\lambda} \) . The procedure is then to divide each eigenvector by this quantity, and it is in this step that generates discrepancy, as one may divide by \( -1/\sqrt{\lambda} \) getting the same \( \mathcal{L}_2 \)-norm.

Sklearn's implementation uses an auxiliary function \texttt{svd\_flip} that deterministically assigns a sign for each normalized eigenvector (\href{https://github.com/scikit-learn/scikit-learn/blob/15c2c72e27c6ea18566f4e786506c7a3aef8a5de/sklearn/utils/extmath.py#L504}{reference}). 

We have implemented this option as a parameter for our function in order to reproduce Sklearn's output perfectly. In our case, not using this function leads to a reflection of the second principal component in the Kernel PCA function. Linear PCA does not show this behavior by pure coincidence.

\emph{Vary the parameters of the kernel and comment on the behavior of the projections onto the first two KPCA components for the different values considered (e.g. $\gamma \in \left\{0.02, 0.2, 2.0, 20.0, 200.0, 2000.0\right\}$. In particular,}
\begin{enumerate}
    \item \emph{What is the behaviour in the limit in which the width of the kernel approaches $\infty$. Explain why one should expect such behavior.}
    \item \emph{What is the behaviour in the limit in which the width of the kernel approaches $0$. Explain why one should expect such behavior.}
\end{enumerate}

First of all, to study the limit behaviour of the projections we are using the explicit kernel formula:
\[
     \mathcal{K}(x, y) = Ae^{-\frac{\|x-y\|^{2}}{2 \omega^2}}, 
\] 
where \( A \) is the output variance and \( \omega \) is the kernel's width and \( \gamma \) is inversely proportional to \( \omega \) . Using this formula, and given two fixed input values \( x \) and \( y \),
\[
     \lim_{\gamma \to 0^{+}} = \lim_{\omega \to \infty} \mathcal{K}(x,y) = A.
\]

\[
     \lim_{\gamma \to \infty} = \lim_{\omega \to 0^{+}} \mathcal{K}(x,y) = 0.
\]

Using this theoretical results, increasing the value of gamma result in projecting all the datapoints into \( 0 \), and decreasing it 

\section*{Nyström approximation}

\emph{Comment on the values of the error for the different approximations, and their dependence with the number of sampled features.}

\emph{(Extra point) Determine de dependence of the mean error with the number of features for the different random feature models. Provide an explanation of this behavior.}


\section{CV}

\textbf{DUDA:}¿adaptamos nuestro código para poder usarlo con sklearn? (heredar de BaseEstimator, pasar los parámetros en el constructor, que el fit sea fit de verdad, etc...)

\section*{References}

Formato Chicago de todos los trabajos consultados + sklearn.

\section*{Appendix: code developed}

\textbf{DUDA:} ¿Qué código hay que poner aquí?

\end{document}
