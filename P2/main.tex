\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=magenta}
\setlength{\parindent}{0in}
\usepackage[margin=0.8in]{geometry}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{engord}
\usepackage{parskip}
% \usepackage{minted}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[compact]{titlesec}
\usepackage[center]{caption}
\usepackage{placeins}
\usepackage{color}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{pdfpages}
\usepackage{bm}
% \titlespacing*{\subsection}{0pt}{5.5ex}{3.3ex}
% \titlespacing*{\section}{0pt}{5.5ex}{1ex}
\author{Luis Antonio Ortega Andrés\\Antonio Coín Castro}
\date{\today}
\title{Kernel Methods\\\medskip
\large Homework 2}
\hypersetup{
 pdfauthor={Luis Antonio Ortega Andrés, Antonio Coín Castro},
 pdftitle={HW02},
 pdfkeywords={},
 pdfsubject={},
 pdflang={English}}

% \usemintedstyle{bw}

\begin{document}

\maketitle

\section{TODO}

Contestar preguntas en notebook (y eliminar este pdf).

Pasar .pys a pdf.

Añadir referencias (documentación sklearn, paper PCA?) en formato Chicago.

Aclarar que eliminamos las componentes no nulas en KPCA.

Añadir gráficas sobre cómo es KPCA según gamma->0 o gamma->inf (están en la carpeta /img) (?)

En el notebook de CV:

- Comentar gráfica.

- Comentar mejores parámetros y resultado en test.

\section*{Kernel PCA}

\emph{Why do the projections onto the first two KPCA principal components look different for the Sklearn and our implementation? Is any of the two incorrect?}

Both representations are correct and valid, what makes them different is the decisions made at normalizing the eigenvectors.

These vectors are returned with \(\|\cdot \|_{\mathcal{L}_2} = 1\) in \( \mathbb{R}^N \)  by Numpy and Scipy via specialized functions such as \texttt{eig} and \texttt{eigh} (version for symmetric cases). However, the kernel method needs these vectors to be unitary in the subjacent Hilbert space, not \( \mathbb{R}^N \). This can be achieved by ensuring that their \( \mathcal{L}_2 \)-norm in \( \mathbb{R}^N \)  equals the inverse of the square root of their corresponding eigenvalues \( 1/\sqrt{\lambda} \) . The procedure is then to divide each eigenvector by this quantity, and it is in this step that generates discrepancy, as one may divide by \( -1/\sqrt{\lambda} \) getting the same \( \mathcal{L}_2 \)-norm.

Sklearn's implementation uses an auxiliary function \texttt{svd\_flip} that deterministically assigns a sign for each normalized eigenvector (\href{https://github.com/scikit-learn/scikit-learn/blob/15c2c72e27c6ea18566f4e786506c7a3aef8a5de/sklearn/utils/extmath.py#L504}{reference}).

We have implemented this option as a parameter for our function in order to reproduce Sklearn's output perfectly. In our case, not using this function leads to a reflection of the second principal component in the Kernel PCA function. Linear PCA does not show this behavior by pure coincidence.

\emph{Vary the parameters of the kernel and comment on the behavior of the projections onto the first two KPCA components for the different values considered (e.g. $\gamma \in \left\{0.02, 0.2, 2.0, 20.0, 200.0, 2000.0\right\}$. In particular,}
\begin{enumerate}
    \item \emph{What is the behavior in the limit in which the width of the kernel approaches $\infty$. Explain why one should expect such behavior.}
    \item \emph{What is the behavior in the limit in which the width of the kernel approaches $0$. Explain why one should expect such behavior.}
\end{enumerate}

First of all, to study the limit behavior of the projections we are using the explicit kernel formula:
\[
     \mathcal{K}(x, y) = Ae^{-\frac{\|x-y\|^{2}}{2 \omega^2}},
\]
where \( A \) is the output variance and \( \omega \) is the kernel's width and \( \gamma \) is inversely proportional to \( \omega \) . Using this formula, and given two fixed input values \( x \neq y \),
\[
     \lim_{\gamma \to 0^{+}} = \lim_{\omega \to \infty} \mathcal{K}(x,y) = A.
\]

\[
     \lim_{\gamma \to \infty} = \lim_{\omega \to 0^{+}} \mathcal{K}(x,y) = 0.
\]

Let us begin with the upper limit, in this case, the kernel tends to \( 0 \) on any pair of non equal points. This does not apply when computing \( \lim_{\gamma \to \infty} \mathcal{K}(x,x) \) as \( \mathcal{K}(x,x) = 1 \) is constant given any value of \( \gamma \). Given this situation and considering a set of points \( \bm{X} \), their kernel matrix converges to
\[
      \lim_{\gamma \to \infty} \mathcal{K}(\bm{X}, \bm{X}) = \bm{I}.
\]
This leads to the following situation: the application of the kernel to any set of points results in an identity matrix, which has all of its eigenvalues equal to \( 1 \) and its eigenvector to settle a base in \( \mathbb{R}^N \). There is no need for such base to be the usual base of \( \mathbb{R}^N \) but it does when using Numpy's \texttt{eigh} function.

As a result, the kernel method performs the following matrix operation:
\[
      \mathcal{K}(\bm{X}_{test}, \bm{X})\begin{pmatrix} e_1 & \dots & e_N \end{pmatrix}
\]

Which would lead to a \( 0 \) matrix if \( \bm{X}_{test} \) and \( \bm{X} \) where disjoint. However, in our case, these matrix are equal, and their kernel a identity matrix:
\[
      \mathcal{K}(\bm{X}, \bm{X})\begin{pmatrix} e_1 & \dots & e_N \end{pmatrix} = \bm{I}\bm{I} =  \bm{I}.
\]
Given this, plotting a projection over the first two components leads to plotting a point in \( (1,0) \), a points in \( (0,1) \) and the rest in the origin. Which is what can be seen at the end of the animation.

However, there is an interesting behaviour of the \texttt{eigh} function when its argument is near to an identity matrix but it is not. The returned eigenvalues are roughly equal to \( 1 \) and the eigenvector are nearly a base of \( \mathbb{R}^N \), but they are not the usual base (neither near). As a result, the plot leads to the projection over \( \mathbb{R}^2\) of a base in \( \mathbb{R}^N \), which corresponds to the ``spiked''  appearance of the projection.




Using this theoretical results, increasing the value of gamma result in projecting all the datapoints into \( 0 \), and decreasing it

\section*{Nyström approximation}

\emph{Comment on the values of the error for the different approximations, and their dependence with the number of sampled features.}

\emph{(Extra point) Determine de dependence of the mean error with the number of features for the different random feature models. Provide an explanation of this behavior.}
\end{document}
