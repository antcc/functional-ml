\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=magenta}
\setlength{\parindent}{0in}
\usepackage[margin=0.8in]{geometry}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{engord}
\usepackage{parskip}
\usepackage{minted}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[compact]{titlesec}
\usepackage[center]{caption}
\usepackage{placeins}
\usepackage{color}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{minted}
\usepackage{subfigure}
\usepackage{pdfpages}
% \titlespacing*{\subsection}{0pt}{5.5ex}{3.3ex}
% \titlespacing*{\section}{0pt}{5.5ex}{1ex}
\author{Luis Antonio Ortega Andrés\\Antonio Coín Castro}
\date{\today}
\title{Kernel Methods\\\medskip
\large Homework 2}
\hypersetup{
 pdfauthor={Luis Antonio Ortega Andrés, Antonio Coín Castro},
 pdftitle={HW02},
 pdfkeywords={},
 pdfsubject={},
 pdflang={English}}

% \usemintedstyle{bw}

\begin{document}

\maketitle

\section{Kernel PCA}

2. Why do the projections onto the first two KPCA principal components look different for the sklearn and our implementation? Is any of the two incorrect?

Los vectores alpha tienen norma (l2) igual a 1 en $\mathbb R^N$, y se demuestra que si conseguimos que tengan norma (l2) $1/\sqrt{\lambda_i}$ entonces los autovectores $V_i$ definidos por $\alpha_i$ en el espacio $H$ tendrán norma-H igual a 1. Esto se reduce a dividir $\alpha_i/\sqrt{\lambda_i}$.

Las dos representaciones mostradas son correctas, solo que una está usando los vectores alpha cambiados de signo (siempre hay dos posibilidades). Sklearn usa la función svd\_flip para unificar la elección de signo (\url{https://github.com/scikit-learn/scikit-learn/blob/15c2c72e27c6ea18566f4e786506c7a3aef8a5de/sklearn/utils/extmath.py#L504}). El efecto visual es una simetría, ya que en este caso cambia solo el autovector correspondiente a la segunda componente principal.

En PCA lineal no cambia por casualidad.

\textbf{DUDA}: ¿qué hacemos con las componentes/autovalores nulos? ¿Devolvemos columnas con todo 0s? ¿O las eliminamos antes? ¿O no hacemos nada?

3. Vary the parameters of the kernel and comment on the behavior of the projections onto the first two KPCA components for the different values considered (e.g. $\gamma \in \left\{0.02, 0.2, 2.0, 20.0, 200.0, 2000.0\right\}$). In particular,

    a) What is the behaviour in the limit in which the width of the kernel approaches $\infty$. Explain why one should expect such behavior.

    b) What is the behaviour in the limit in which the width of the kernel approaches $0$. Explain why one should expect such behavior.

\section{Nyström approximation}

2. Comment on the values of the error for the different approximations, and their dependence with the number of sampled features.

3. (Extra point) Determine de dependence of the mean error with the number of features for the different random feature models. Provide an explanation of this behavior.

\textbf{DUDA:} Qué hay que hacer exactamente con Xprime en Nystrom. Para qué sirve la función approximate\_kernel\_matrix? ¿No sería mejor que aceptara X y X\_prime?

\textbf{DUDA:} ¿Hay que entregar lo de mnist?

\section*{Appendix: code developed}

\end{document}
